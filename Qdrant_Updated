import hashlib
import ftplib
import gzip
from io import BytesIO
import os
import ollama
import xml.etree.ElementTree as ET
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import logging

# FTP server details
ftp_server = "ftp.ncbi.nlm.nih.gov"
ftp_directory = "/pubmed/baseline/"
file_pattern = "pubmed24n{:04d}.xml.gz"
md5_file_pattern = "pubmed24n{:04d}.xml.gz.md5"
checkpoint_file = "processed_files.txt"

# Setup logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler("error_log.log"), logging.StreamHandler()])

# Qdrant client setup
qdrant_client = QdrantClient(host='localhost', port=6333)


def read_checkpoint():
    """Read the list of processed files from the checkpoint file."""
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            return set(line.strip() for line in f)
    return set()


def update_checkpoint(file_name):
    """Append the processed file name to the checkpoint file."""
    with open(checkpoint_file, 'a') as f:
        f.write(f"{file_name}\n")


def generate_bgem3_embedding(text, model='bge-m3'):
    try:
        response = ollama.embeddings(model=model, prompt=text)
        return response['embedding']
    except Exception as e:
        logging.error(f"Error generating bge-m3 embedding: {e}")
        return None

def generate_bge_large_embedding(text, model='bge-large'):
    try:
        response = ollama.embeddings(model=model, prompt=text)
        return response['embedding']
    except Exception as e:
        logging.error(f"Error generating bge-large embedding: {e}")
        return None

def ensure_collection_exists(client, collection_name):
    try:
        if not client.collection_exists(collection_name):
            client.create_collection(
                collection_name=collection_name,
                vectors_config={
                    "openai_embedding": VectorParams(size=1536, distance=Distance.COSINE),
                    "bgem3_embedding": VectorParams(size=1024, distance=Distance.COSINE),
                    "bge_large_embedding": VectorParams(size=1024, distance=Distance.COSINE)
                }
            )
    except Exception as e:
        logging.error(f"Error ensuring collection exists: {e}")

def parse_pubmed_articles(data, max_articles): 
    try:
        root = ET.fromstring(data)
        articles_data = []
        article_count = 0

        for medline_citation in root.findall(".//MedlineCitation"):
            if article_count >= max_articles:
                break
            
            article_data = {}

            # Extract CommentsCorrections (to handle updates and retractions)
            comments_corrections = medline_citation.findall(".//CommentsCorrections")
            is_retracted = any(comment.attrib.get('RefType', '') in ["Retraction of", "Retraction in"] for comment in comments_corrections)
            if is_retracted:
                continue  # Skip retracted articles
            
            # Extract Abstract
            abstract_elements = medline_citation.findall(".//Abstract/AbstractText")
            if abstract_elements:
                abstract_texts = [abstract.text for abstract in abstract_elements if abstract.text]
                article_data['Abstract'] = ' '.join(abstract_texts)
            else:
                continue  # Skip articles without an abstract

            # Extract PMID
            article_data['PMID'] = medline_citation.find("PMID").text if medline_citation.find("PMID") is not None else ''
            article_data['PMID_Version'] = medline_citation.find("PMID").attrib.get('Version', '') if medline_citation.find("PMID") is not None else ''

            # Extract Journal Information
            journal = medline_citation.find(".//Journal")
            if journal is not None:
                article_data['Journal'] = {
                    'Title': journal.find("Title").text if journal.find("Title") is not None else '',
                    'Volume': journal.find(".//JournalIssue/Volume").text if journal.find(".//JournalIssue/Volume") is not None else '',
                    'PubDate': {
                        'Year': journal.find(".//JournalIssue/PubDate/Year").text if journal.find(".//JournalIssue/PubDate/Year") is not None else '',
                        'Month': journal.find(".//JournalIssue/PubDate/Month").text if journal.find(".//JournalIssue/PubDate/Month") is not None else '',
                        'Day': journal.find(".//JournalIssue/PubDate/Day").text if journal.find(".//JournalIssue/PubDate/Day") is not None else ''
                    }
                }
            else:
                article_data['Journal'] = {
                    'Title': '',
                    'Volume': '',
                    'PubDate': {
                        'Year': '',
                        'Month': '',
                        'Day': ''
                    }
                }

            # Extract Article Title
            article_data['Title'] = medline_citation.find(".//ArticleTitle").text if medline_citation.find(".//ArticleTitle") is not None else ''

            # Extract Authors
            authors = medline_citation.findall(".//AuthorList/Author")
            complete_yn = medline_citation.find(".//AuthorList").attrib.get('CompleteYN', 'Y')
            author_list = []
            for author in authors:
                author_data = {
                    'LastName': author.find("LastName").text if author.find("LastName") is not None else '',
                    'ForeName': author.find("ForeName").text if author.find("ForeName") is not None else '',
                }
                author_list.append(author_data)
            
            article_data['Authors'] = author_list
            article_data['AuthorsComplete'] = complete_yn == 'Y'

            # Extract Keywords
            keywords = medline_citation.findall(".//Keyword")
            article_data['Keywords'] = [keyword.text for keyword in keywords if keyword.text]

            # Extract Publication Identifiers
            article_data['PublicationIdentifiers'] = {
                'DOI': medline_citation.find(".//ELocationID[@EIdType='doi']").text if medline_citation.find(".//ELocationID[@EIdType='doi']") is not None else '',
            }

            articles_data.append(article_data)
            article_count += 1

        return articles_data

    except ET.ParseError as e:
        logging.error(f"Error parsing PubMed XML: {e}")
        return []

def generate_payload(article_data):
    try:
        # Generate text embeddings for the abstract
        bgem3_embedding = generate_bgem3_embedding(article_data['Abstract'])  
        bge_large_embedding = generate_bge_large_embedding(article_data['Abstract'])

        payload = {
            "pmid": article_data['PMID'],
            "pmid_version": article_data['PMID_Version'],
            "title": article_data['Title'],
            "abstract": article_data['Abstract'],
            "authors": article_data['Authors'],
            "authors_complete": article_data['AuthorsComplete'],
            "journal": {
                "title": article_data['Journal']['Title'],
                "volume": article_data['Journal']['Volume'],
                "pub_date": {
                    "year": article_data['Journal']['PubDate']['Year'],
                    "month": article_data['Journal']['PubDate']['Month'],
                    "day": article_data['Journal']['PubDate']['Day']
                }
            },
            "keywords": article_data['Keywords'],
            "publication_identifiers": article_data['PublicationIdentifiers'], 
            "bgem3_embedding": bgem3_embedding,
            "bge_large_embedding": bge_large_embedding    
        }
        return payload
    except Exception as e:
        logging.error(f"Error generating payload: {e}")
        return None

def insert_payload(client, payload, collection_name):
    try:
        point = PointStruct(id=int(payload['pmid']), vector={"bgem3_embedding": payload['bgem3_embedding'], "bge_large_embedding": payload["bge_large_embedding"]}, payload=payload) 
        response = client.upsert(collection_name=collection_name, points=[point])
        return response
    except Exception as e:
        logging.error(f"Error inserting payload to Qdrant: {e}")
        return None
def process_and_upload(file_name, compressed_data, collection_name):
    try:
        with gzip.GzipFile(fileobj=compressed_data, mode='rb') as f_in:
            extracted_data = f_in.read()
        
        articles_data = parse_pubmed_articles(extracted_data, max_articles=2)  # Adjust max_articles as needed
        if articles_data:
            for article_data in articles_data:
                payload = generate_payload(article_data)
                if payload:
                    response = insert_payload(qdrant_client, payload, collection_name=collection_name)
                    logging.info(f"Inserted article PMID {article_data['PMID']} to Qdrant.")
        
        # If we reach this point, all abstracts in the file were processed successfully.
        return True
    except Exception as e:
        logging.error(f"Error processing and uploading file {file_name}: {e}")
        return False

def main():
    try:
        ftp = ftplib.FTP(ftp_server)
        ftp.login()
        ftp.cwd(ftp_directory)

        collection_name = "PubMed"
        ensure_collection_exists(qdrant_client, collection_name)

        # Read checkpoint
        processed_files = read_checkpoint()

        for i in range(1, 2):  # Adjust range up to 1220 for the full dataset.
            file_name = file_pattern.format(i)
            md5_file_name = md5_file_pattern.format(i)

            # Skip already processed files
            if file_name in processed_files:
                logging.info(f"Skipping already processed file {file_name}")
                continue
            
            # Retrieve and check MD5
            md5_data = BytesIO()
            ftp.retrbinary(f"RETR {md5_file_name}", md5_data.write)
            md5_contents = md5_data.getvalue().decode().strip()
            expected_md5 = md5_contents.split('=')[1].strip() if '=' in md5_contents else md5_contents.split()[0].strip()

            # Retrieve the compressed file
            compressed_data = BytesIO()
            ftp.retrbinary(f"RETR {file_name}", compressed_data.write)
            
            # Calculate MD5 for the compressed data
            calculated_md5 = hashlib.md5(compressed_data.getvalue()).hexdigest()

            if calculated_md5 == expected_md5:
                compressed_data.seek(0)  # Reset buffer position
                logging.info(f"Checksums matched. Processing file {file_name}...")
                if process_and_upload(file_name, compressed_data, collection_name):
                    # Update checkpoint after successful processing of the entire file
                    update_checkpoint(file_name)
            else:
                logging.warning(f"MD5 mismatch for file {file_name}. Expected: {expected_md5}, Calculated: {calculated_md5}")

        ftp.quit()
    except Exception as e:
        logging.error(f"Error in main FTP process: {e}")
